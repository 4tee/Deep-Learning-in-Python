# Making predictions #

The trained network from your previous coding exercise is now stored as `model`. New data to make predictions is stored in a NumPy array as `pred_data`. Use `model` to make predictions on your new data.

In this exercise, your predictions will be probabilities, which is the most common way for data scientists to communicate their predictions to colleagues.

## Instructions ##

* Create your predictions using the model's `.predict()` method on `pred_data`.
* Use NumPy indexing to find the column corresponding to predicted probabilities of survival being `True`. This is the second column (index `1`) of `predictions`. Store the result in `predicted_prob_true` and print it.

```python
# Specify, compile, and fit the model
model = Sequential()
model.add(Dense(32, activation='relu', input_shape = (n_cols,)))
model.add(Dense(2, activation='softmax'))
model.compile(optimizer='sgd', 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])
model.fit(predictors, target)

# Calculate predictions: predictions
predictions = model.predict(pred_data)

# Calculate predicted probability of survival: predicted_prob_true
predicted_prob_true = predictions[:,1]

# print predicted_prob_true
print(predicted_prob_true)
```

```
    Epoch 1/10
    
 32/800 [>.............................] - ETA: 0s - loss: 4.3188 - acc: 0.6562
736/800 [==========================>...] - ETA: 0s - loss: 3.0339 - acc: 0.5924
800/800 [==============================] - 0s - loss: 2.9202 - acc: 0.5887     
    Epoch 2/10
    
 32/800 [>.............................] - ETA: 0s - loss: 0.5366 - acc: 0.7812
736/800 [==========================>...] - ETA: 0s - loss: 1.2165 - acc: 0.6359
800/800 [==============================] - 0s - loss: 1.1632 - acc: 0.6425     
    Epoch 3/10
    
 32/800 [>.............................] - ETA: 0s - loss: 0.8220 - acc: 0.6250
704/800 [=========================>....] - ETA: 0s - loss: 0.7489 - acc: 0.6477
800/800 [==============================] - 0s - loss: 0.8065 - acc: 0.6412     
    Epoch 4/10
    
 32/800 [>.............................] - ETA: 0s - loss: 0.5342 - acc: 0.7812
736/800 [==========================>...] - ETA: 0s - loss: 0.6999 - acc: 0.6522
800/800 [==============================] - 0s - loss: 0.6950 - acc: 0.6487     
    Epoch 5/10
    
 32/800 [>.............................] - ETA: 0s - loss: 0.6756 - acc: 0.4688
736/800 [==========================>...] - ETA: 0s - loss: 0.6312 - acc: 0.6590
800/800 [==============================] - 0s - loss: 0.6194 - acc: 0.6663     
    Epoch 6/10
    
 32/800 [>.............................] - ETA: 0s - loss: 0.6479 - acc: 0.6250
736/800 [==========================>...] - ETA: 0s - loss: 0.6596 - acc: 0.6603
800/800 [==============================] - 0s - loss: 0.6429 - acc: 0.6687     
    Epoch 7/10
    
 32/800 [>.............................] - ETA: 0s - loss: 0.5943 - acc: 0.6562
736/800 [==========================>...] - ETA: 0s - loss: 0.6327 - acc: 0.6644
800/800 [==============================] - 0s - loss: 0.6306 - acc: 0.6625     
    Epoch 8/10
    
 32/800 [>.............................] - ETA: 0s - loss: 0.7355 - acc: 0.6250
736/800 [==========================>...] - ETA: 0s - loss: 0.6113 - acc: 0.6780
800/800 [==============================] - 0s - loss: 0.6046 - acc: 0.6800     
    Epoch 9/10
    
 32/800 [>.............................] - ETA: 0s - loss: 0.5808 - acc: 0.7500
736/800 [==========================>...] - ETA: 0s - loss: 0.5966 - acc: 0.6807
800/800 [==============================] - 0s - loss: 0.5966 - acc: 0.6800     
    Epoch 10/10
    
 32/800 [>.............................] - ETA: 0s - loss: 0.6054 - acc: 0.7812
704/800 [=========================>....] - ETA: 0s - loss: 0.5627 - acc: 0.7074
800/800 [==============================] - 0s - loss: 0.5739 - acc: 0.7013     
    [0.37773594 0.38779387 0.8617933  0.41962516 0.23589827 0.2305667
     0.09094229 0.4410792  0.3428099  0.55171555 0.2610304  0.2831983
     0.30674142 0.46145362 0.23893961 0.15584648 0.40823746 0.42664024
     0.14664881 0.43603414 0.61188275 0.27243263 0.09435254 0.4777252
     0.44202286 0.22480863 0.53260565 0.58819926 0.24115254 0.5830221
     0.4332432  0.47570226 0.26981765 0.27819124 0.320638   0.6758633
     0.30994806 0.27151197 0.51950675 0.42359298 0.29851532 0.36992088
     0.49905822 0.19876142 0.33647117 0.16957311 0.46900314 0.22704908
     0.40734583 0.79103094 0.41134202 0.06878198 0.4835788  0.6247207
     0.35244992 0.4302664  0.93833965 0.34790307 0.30278853 0.26981765
     0.2680113  0.40423694 0.380417   0.49318254 0.39206216 0.45817885
     0.49361077 0.5311783  0.30552164 0.43803102 0.26121357 0.45402882
     0.23145582 0.17254047 0.43351546 0.4736752  0.34644276 0.3041968
     0.26803815 0.5851909  0.42096362 0.2223628  0.48886666 0.35363033
     0.24860387 0.22440806 0.40565234 0.5284093  0.30991668 0.47583154
     0.20368876]
```